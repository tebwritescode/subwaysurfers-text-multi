# Memory-Optimized Docker Compose Configuration
# Based on memory profiling analysis for Bark TTS integration
# Subway Surfers Text-to-Video Generator

version: '3.8'

services:
  # Development/Testing - Mock TTS Server
  pytorch-tts-mock:
    build:
      context: ./pytorch-tts-server
      dockerfile: Dockerfile
    command: ["python", "app_mock.py"]
    ports:
      - "8000:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - LOG_LEVEL=info
    deploy:
      resources:
        limits:
          memory: 512M      # Based on profiling: 208MB + safety margin
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Production - Small Bark Model
  pytorch-tts-small:
    build:
      context: ./pytorch-tts-server
      dockerfile: Dockerfile
    command: ["python", "app.py"]
    ports:
      - "8001:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - LOG_LEVEL=info
      - SUNO_USE_SMALL_MODELS=True
      - SUNO_OFFLOAD_CPU=True
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048
      - TRANSFORMERS_CACHE=/app/cache
    volumes:
      - ./cache:/app/cache
      - /tmp:/tmp
    deploy:
      resources:
        limits:
          memory: 6G        # Based on estimate: 3.4GB + overhead
          cpus: '4.0'
        reservations:
          memory: 2G
          cpus: '2.0'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s    # Model loading takes time

  # Production - Full Bark Model (GPU)
  pytorch-tts-full-gpu:
    build:
      context: ./pytorch-tts-server
      dockerfile: Dockerfile
    command: ["python", "app.py"]
    ports:
      - "8002:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - LOG_LEVEL=info
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:4096
      - TRANSFORMERS_CACHE=/app/cache
    volumes:
      - ./cache:/app/cache
      - /tmp:/tmp
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 12G       # Based on estimate: 6.3GB GPU + 4GB system
          cpus: '8.0'
        reservations:
          memory: 6G
          cpus: '4.0'
        generic_resources:
          - discrete_resource_spec:
              kind: 'NVIDIA-GPU'
              value: 1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s    # GPU model loading takes more time

  # Production - Full Bark Model (CPU Only)
  pytorch-tts-full-cpu:
    build:
      context: ./pytorch-tts-server
      dockerfile: Dockerfile
    command: ["python", "app.py"]
    ports:
      - "8003:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - LOG_LEVEL=info
      - SUNO_OFFLOAD_CPU=True
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048
      - TRANSFORMERS_CACHE=/app/cache
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
    volumes:
      - ./cache:/app/cache
      - /tmp:/tmp
    deploy:
      resources:
        limits:
          memory: 16G       # Based on estimate: 8.9GB + overhead
          cpus: '12.0'
        reservations:
          memory: 8G
          cpus: '6.0'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s    # CPU model loading is slowest

  # Load Balancer for High Availability
  nginx-lb:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./nginx-lb.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - pytorch-tts-small
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.5'
        reservations:
          memory: 64M
          cpus: '0.25'
    restart: unless-stopped

  # Memory Monitoring (Optional)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    restart: unless-stopped

volumes:
  prometheus_data:
    driver: local
  cache:
    driver: local

networks:
  default:
    driver: bridge

# Additional configurations for memory optimization
x-memory-optimized-base: &memory-optimized-base
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nproc: 65535
    nofile:
      soft: 65535
      hard: 65535
  security_opt:
    - seccomp:unconfined
  tmpfs:
    - /tmp:noexec,nosuid,size=1G